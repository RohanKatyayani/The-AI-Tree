# Ensemble Methods

### One-Liner:
*   **What it is:** Machine learning techniques that combine multiple base models to produce one optimal predictive model.

### The Big Picture:
Ensemble Methods are the **wisdom of the crowd** applied to machine learning. Instead of relying on a single model, they leverage the strengths of multiple models while mitigating their individual weaknesses.

### How it Works (The Core Idea):
The core principle is that a group of "weak learners" (simple models) can come together to form a "strong learner" (powerful model). This works because different models make different types of errors, and when combined, these errors often cancel out.

### Why it Matters:
Ensembles typically achieve higher accuracy, robustness, and generalization than any single model alone. They're among the most reliable techniques in practical machine learning.

### A Simple Analogy:
**Asking a medical diagnosis from multiple specialists.**
*   A **single doctor** might miss something or be biased.
*   A **team of specialists** (cardiologist, neurologist, radiologist) will each bring different expertise.
*   By combining their opinions (**ensembling**), you get a more accurate and reliable diagnosis than from any one doctor alone.

### Real-World Examples:
*   **Random Forests:** Combining multiple decision trees.
*   **Gradient Boosting Machines:** Sequentially building models that correct previous errors.
*   **Model stacking:** Using predictions from multiple models as features for a final model.

---
*🌳 **A Foundational Technique:** In the [[Machine Learning]] toolkit.
*🤝 **The Core Idea:** **Combining models** > single models.
*🎯 **The Methods:** **Bagging**, **Boosting**, **Stacking**.
*🚀 **The Result:** **Better performance** and **robustness**.
