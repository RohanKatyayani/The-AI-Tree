# Test Set

### One-Liner:
*   **What it is:** A portion of the dataset that is held back and *never used* during training, reserved solely for the final evaluation of the model's performance.

### The Big Picture:
The Test Set is the **final, unbiased exam** for a machine learning model. It is the gold standard for estimating how well the model will perform on new, real-world data it encounters after deployment.

### How it Works (The Core Idea):
When preparing data, it is split (typically 80/20 or 70/30) into:
*   **Training Set:** Used to teach the model.
*   **Validation Set:** Used to tune its parameters.
*   **Test Set:** Used **only once**, at the very end, to get a final, unbiased performance score.

### Why it Matters:
Using the test set for anything other than final evaluation would be "cheating." It would cause **data leakage** and give an overly optimistic, unrealistic performance estimate, dooming the model to fail in production.

### A Simple Analogy:
**A final exam that is kept locked in a safe.**
*   The **Training Set** is your textbook and practice problems.
*   The **Test Set** is the sealed final exam.
*   If you peek at the exam (use the test set during training), your grade is meaningless. You only open it once, on exam day, to get a true measure of what you've learned.

### Real-World Examples:
*   A model predicting stock prices is evaluated on its performance on data from future dates it wasn't trained on.
*   A medical diagnosis AI is tested on scans from new patients to see if it generalizes.

---
*üå≥ **The Final Judge:** Provides the data for calculating [[Evaluation Metrics]].
*‚ö†Ô∏è **The Rule:** Must **never** be used during [[Training]] or model selection.
*üìä **Part Of:** The broader practice of [[Train-Validation-Test Split]].
*üîí **Prevents:** **Data Leakage**, which leads to false confidence and model failure.
